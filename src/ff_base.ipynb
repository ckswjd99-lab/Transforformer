{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Forward-Forward Models\n",
    "\n",
    "Codes mainly referenced from: https://github.com/carloalbertobarbano/forward-forward-pytorch\n",
    "\n",
    "1. Training base FF model, presented in paper: https://arxiv.org/abs/2212.13345\n",
    "\n",
    "2. Training modified FF model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import forward_forward as ff\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import argparse\n",
    "import torch.utils.tensorboard\n",
    "\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HYPERPARAMS ##\n",
    "\n",
    "HARD_NEGATIVES = True\n",
    "LAYER_SIZE = 2000\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "LEARNING_RATE = 0.0001\n",
    "WEIGHT_DECAY = 0\n",
    "NUM_EPOCHS = 60\n",
    "STEPS_PER_BLOCK = 60\n",
    "THETA = 10.\n",
    "\n",
    "SEED = 42\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UTILS ##\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(1.0 / batch_size).item())\n",
    "        return res\n",
    "\n",
    "def norm_y(y_one_hot: torch.Tensor):\n",
    "    return y_one_hot.sub(0.1307).div(0.3081)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRAINING ##\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(network_ff, linear_cf, test_loader, verbose=False):\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "\n",
    "    for (x_test, y_test) in test_loader:\n",
    "        x_test, y_test = x_test.to(DEVICE), y_test.to(DEVICE)\n",
    "        x_test = x_test.view(x_test.shape[0], -1)\n",
    "\n",
    "        acts_for_labels = []\n",
    "\n",
    "        # slow method\n",
    "        for label in range(10):\n",
    "            test_label = torch.ones_like(y_test).fill_(label)\n",
    "            test_label = norm_y(F.one_hot(test_label, num_classes=10))\n",
    "            x_with_labels = torch.cat((x_test, test_label), dim=1)\n",
    "            \n",
    "            acts = network_ff(x_with_labels)\n",
    "            acts = acts.norm(dim=-1)\n",
    "            acts_for_labels.append(acts)\n",
    "        \n",
    "        # these are logits\n",
    "        acts_for_labels = torch.stack(acts_for_labels, dim=1) #should be BSZxLABELSxLAYERS (10)\n",
    "        all_outputs.append(acts_for_labels)\n",
    "        all_labels.append(y_test)\n",
    "\n",
    "        # quick method\n",
    "        neutral_label = norm_y(torch.full((x_test.shape[0], 10), 0.1, device=DEVICE))\n",
    "        acts = network_ff(torch.cat((x_test, neutral_label), dim=1))\n",
    "        logits = linear_cf(acts.view(acts.shape[0], -1))\n",
    "        all_logits.append(logits)\n",
    "\n",
    "    all_outputs = torch.cat(all_outputs)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    all_logits = torch.cat(all_logits)\n",
    "\n",
    "    slow_acc = accuracy(all_outputs.mean(dim=-1), all_labels, topk=(1,))[0]\n",
    "    fast_acc = accuracy(all_logits, all_labels, topk=(1,))[0]\n",
    "    return slow_acc, fast_acc\n",
    "\n",
    "def train(network_ff, optimizer, linear_cf, optimizer_cf, train_loader, start_block):\n",
    "    running_loss = 0.\n",
    "    running_ce = 0.\n",
    "\n",
    "    for (x, y_pos) in train_loader:\n",
    "        x, y_pos = x.to(DEVICE), y_pos.to(DEVICE)\n",
    "        x = x.view(BATCH_SIZE, -1)\n",
    "\n",
    "        # positive pairs\n",
    "        y_pos_one_hot = norm_y(F.one_hot(y_pos, num_classes=10))\n",
    "        x_pos = torch.cat((x, y_pos_one_hot), dim=1)\n",
    "        \n",
    "        # sample negatives (and train linear cf)\n",
    "        with torch.no_grad():\n",
    "            ys = network_ff(torch.cat((x, torch.ones_like(y_pos_one_hot).fill_(0.1)), dim=1))\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            logits = linear_cf(ys.view(ys.shape[0], -1).detach())\n",
    "            ce = F.cross_entropy(logits, y_pos)\n",
    "            ce.backward()\n",
    "            running_ce += ce.detach()\n",
    "\n",
    "        optimizer_cf.step()\n",
    "        optimizer_cf.zero_grad()\n",
    "\n",
    "        # negative pairs from softmax layer\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "        idx = torch.where(preds != y_pos)\n",
    "        y_hard_one_hot = norm_y(F.one_hot(preds, num_classes=10))\n",
    "        x_hard = torch.cat((x, y_hard_one_hot), dim=1)[idx]\n",
    "\n",
    "        # negative pairs from random labels\n",
    "        y_rand = torch.randint(0, 10, (BATCH_SIZE,), device=DEVICE)\n",
    "        idx = torch.where(y_rand != y_pos) # correct labels\n",
    "        y_rand_one_hot = norm_y(F.one_hot(y_rand, num_classes=10))\n",
    "        x_rand = torch.cat((x, y_rand_one_hot), dim=1) #[idx] # keeping positives seems to work better\n",
    "\n",
    "        x_neg = x_rand\n",
    "        if HARD_NEGATIVES:\n",
    "            x_neg = torch.cat((x_neg, x_hard), dim=0)\n",
    "            \n",
    "        with torch.enable_grad():\n",
    "            z_pos = network_ff(x_pos, cat=False)\n",
    "            z_neg = network_ff(x_neg, cat=False)\n",
    "\n",
    "            for idx, (zp, zn) in enumerate(zip(z_pos, z_neg)):\n",
    "                if idx < start_block:\n",
    "                    continue\n",
    "\n",
    "                positive_loss = torch.log(1 + torch.exp((-zp.norm(dim=-1) + THETA))).mean()\n",
    "                negative_loss = torch.log(1 + torch.exp((zn.norm(dim=-1) - THETA))).mean()\n",
    "                loss = positive_loss + negative_loss\n",
    "                loss.backward()\n",
    "\n",
    "                running_loss += loss.detach()\n",
    "                optimizer[idx].step()\n",
    "                optimizer[idx].zero_grad()\n",
    "    \n",
    "    running_loss /= len(train_loader)\n",
    "    running_ce /= len(train_loader)\n",
    "\n",
    "    return running_loss, running_ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREPARE ##\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "T_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "T_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    MNIST(\"~/data\", train=True, download=True, transform=T_train), \n",
    "    batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=8,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    MNIST(\"~/data\", train=False, download=True, transform=T_test), \n",
    "    batch_size=BATCH_SIZE, shuffle=True, num_workers=8,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "size = LAYER_SIZE\n",
    "network_ff = ff.FFBase(dims=[28*28 + 10, size, size, size, size]).to(DEVICE)\n",
    "print(network_ff)\n",
    "\n",
    "# Create one optimizer for evey relu layer (block)\n",
    "optimizers = [\n",
    "    torch.optim.Adam(block.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        for block in network_ff.blocks.children()\n",
    "] \n",
    "\n",
    "# Softmax layer for predicting classes from embeddings (fast method)\n",
    "linear_cf = nn.Linear(size*network_ff.n_blocks, 10).to(DEVICE)\n",
    "optimizer_cf = torch.optim.Adam(linear_cf.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train Base Model\n",
    "\n",
    "Train base model from paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN ##\n",
    "\n",
    "start_block = 0\n",
    "\n",
    "max_fast_acc = 0\n",
    "max_fast_acc_epoch = 0\n",
    "max_slow_acc = 0\n",
    "max_slow_acc_epoch = 0\n",
    "\n",
    "pbar = tqdm(range(1, NUM_EPOCHS+1), total=NUM_EPOCHS)\n",
    "\n",
    "for step in pbar:\n",
    "    running_loss, running_ce = train(network_ff, optimizers, linear_cf, optimizer_cf,\n",
    "                                        train_loader, start_block)\n",
    "    if step % STEPS_PER_BLOCK == 0:\n",
    "        if start_block+1 < network_ff.n_blocks:\n",
    "            start_block += 1\n",
    "            print(\"Freezing block\", start_block-1)\n",
    "\n",
    "    train_slow_acc, train_fast_acc = test(network_ff, linear_cf, train_loader)\n",
    "    test_slow_acc, test_fast_acc = test(network_ff, linear_cf, test_loader)\n",
    "\n",
    "    pbar.set_postfix({\n",
    "        \"train_fast_acc\": train_fast_acc,\n",
    "        \"train_slow_acc\": train_slow_acc,\n",
    "        \"test_fast_acc\": test_fast_acc,\n",
    "        \"test_slow_acc\": test_slow_acc\n",
    "    })\n",
    "\n",
    "    if test_fast_acc > max_fast_acc:\n",
    "        max_fast_acc = test_fast_acc\n",
    "        max_fast_acc_epoch = step\n",
    "    \n",
    "    if test_slow_acc > max_slow_acc:\n",
    "        max_slow_acc = test_slow_acc\n",
    "        max_slow_acc_epoch = step\n",
    "\n",
    "print(\"Min fast acc:\", max_fast_acc, \"at epoch\", max_fast_acc_epoch)\n",
    "print(\"Min slow acc:\", max_slow_acc, \"at epoch\", max_slow_acc_epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tflite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
